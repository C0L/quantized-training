import re
from collections import defaultdict
from typing import Dict, Tuple, Callable, Any, Union, List

import torch
from torch._export import capture_pre_autograd_graph
from torch.ao.quantization.fx.utils import assert_and_get_unique_device
from torch.fx import GraphModule, Node
from torch.fx.graph import map_arg
from accelerate.big_modeling import infer_auto_device_map
from accelerate.utils import get_max_memory


__all__ = [
    "dispatch_model",
    "dtype_byte_size",
    "get_device_map",
    "get_aten_graph_module",
    "get_node_name_to_scope",
    "insert_align_device_nodes",
    "print_node_scope_tabular",
]


def dtype_byte_size(dtype: torch.dtype):
    """
    Returns the size (in bytes) occupied by one parameter of type `dtype`.

    Example:

    ```py
    >>> dtype_byte_size(torch.float32)
    4
    ```
    """
    if dtype == torch.bool:
        return 1 / 8
    if dtype == "e8m0":
        return 1
    bit_search = re.search(r"[^\d](\d+)(_.*)?$", str(dtype))
    if bit_search is None:
        raise ValueError(f"`dtype` is not a valid dtype: {dtype}.")
    bit_size = int(bit_search.groups()[0])
    return bit_size / 8.0


def get_device_map(model: GraphModule, max_memory=None, verbose=False):
    """
    The default device map generated by Hugging Face Accelerator is suboptimal
    because it places all tensors in the order of model parameters, children,
    and buffers, which may not be used in the same order. This function generates
    a device map based on the order of nodes in the graph, which reflects the
    exact order in which tensors are used.
    """
    if max_memory is None:
        max_memory = get_max_memory(max_memory)

    devices = list(max_memory.keys())
    if "disk" not in devices:
        devices.append("disk")

    device_map = {}
    current_device = 0
    current_memory_used = 0

    named_modules = dict(model.named_modules(remove_duplicate=True))

    nodes_to_treat = [n for n in model.graph.nodes if n.op in ['get_attr', 'call_module']]
    while len(nodes_to_treat) > 0:
        node = nodes_to_treat.pop(0)
        if verbose:
            print(f"\nTreating node {node}.")
        # Assess size needed
        if node.op == 'get_attr':
            tensor = getattr(model, node.target)
            module_size = tensor.numel() * dtype_byte_size(tensor.dtype)
        else:
            module = named_modules[node.target]

            module_size = 0
            for param in module.parameters():
                module_size += param.numel() * dtype_byte_size(param.dtype)

            for buffer in module.buffers():
                module_size += buffer.numel() * dtype_byte_size(buffer.dtype)

        device = devices[current_device]
        current_max_size = max_memory[device] if device != "disk" else None

        if current_max_size is not None and current_memory_used + module_size > current_max_size:
            if verbose:
                print(
                    f"Not enough space on {devices[current_device]} to put {node} (space available "
                    f"{current_max_size-current_memory_used}, module size {module_size})."
                )
            current_device += 1
            assert current_device < len(devices), "Not enough devices to store the model."
            nodes_to_treat = [node] + nodes_to_treat
            current_memory_used = 0
        else:
            if verbose:
                if current_max_size is None:
                    print(f"Putting {node} (size={module_size}) on {devices[current_device]}.")
                else:
                    print(
                        f"Putting {node} (size={module_size}) on {devices[current_device]} "
                        f"(available={current_max_size-current_memory_used})."
                    )
            current_memory_used += module_size
            device_map[node.target] = devices[current_device]

    return device_map


def dispatch_model(model, device_map=None, max_memory=None):
    if device_map is None:
        device_map = infer_auto_device_map(model, max_memory)

    for name, param in model.named_parameters(recurse=False):
        param.data = param.data.to(device_map[name])

    for name, child in model.named_children():
        child.to(device_map[name])

    for name, buffer in model.named_buffers(recurse=False):
        buffer.data = buffer.data.to(device_map[name])


@torch.no_grad()
def insert_align_device_nodes(model, example_args):
    args_iter = iter(example_args)
    env : Dict[str, Node] = {}
    modules = dict(model.named_modules())

    node_to_last_use : Dict[Node, Node] = {}
    user_to_last_uses : Dict[Node, List[Node]] = {}

    def register_last_uses(n: Node, user: Node):
        if n not in node_to_last_use:
            node_to_last_use[n] = user
            user_to_last_uses.setdefault(user, []).append(n)

    for node in reversed(model.graph.nodes):
        map_arg(node.args, lambda n: register_last_uses(n, node))
        map_arg(node.kwargs, lambda n: register_last_uses(n, node))

    def delete_unused_values(user : Node):
        """
        Delete values after their last use. This ensures that values that are
        not used in the remainder of the code are freed and the memory usage
        of the code is optimal.
        """
        nodes_to_delete = user_to_last_uses.get(user, [])
        for n in nodes_to_delete:
            env.pop(n.name, None)

    def load_arg(a):
        return torch.fx.graph.map_arg(a, lambda n: env[n.name])

    def fetch_attr(target):
        return getattr(model, target, None)

    def get_unique_device(nodes):
        get_attr_nodes = [n for n in nodes if n.op == 'get_attr']
        args = load_arg(get_attr_nodes if get_attr_nodes else nodes)
        devices = {t.device for t in args if isinstance(t, torch.Tensor)}
        return devices.pop() if devices else None

    def insert_adaptor(node, user, device):
        value = env[node.name]
        if not isinstance(value, torch.Tensor) or value.device == device:
            return

        to_node = next((
            n for n in node.users if n.target == torch.Tensor.to and n.args[1] == device
        ), None)

        if to_node is None:
            with model.graph.inserting_after(node):
                to_node = model.graph.call_function(torch.Tensor.to, (node, device))

        user.replace_input_with(node, to_node)

        if node_to_last_use[node] == user:
            env.pop(node.name)

        env.setdefault(to_node.name, value.to(device))

        # Update last uses
        node_to_last_use.clear()
        user_to_last_uses.clear()

        for n in reversed(model.graph.nodes):
            map_arg(n.args, lambda arg: register_last_uses(arg, n))
            map_arg(n.kwargs, lambda kwarg: register_last_uses(kwarg, n))

    for node in model.graph.nodes:
        if node.op == 'placeholder':
            result = next(args_iter, None)
        elif node.op == 'get_attr':
            result = fetch_attr(node.target)
        elif node.op == 'call_function':
            device = get_unique_device(node.all_input_nodes)
            if device is not None:
                for n in node.all_input_nodes:
                    insert_adaptor(n, node, device)
            result = node.target(*load_arg(node.args), **load_arg(node.kwargs))
        elif node.op == 'call_module':
            module = modules[node.target]
            device = assert_and_get_unique_device(module)
            for n in node.all_input_nodes:
                insert_adaptor(n, node, device)
            result = modules[node.target](*load_arg(node.args), **load_arg(node.kwargs))

        env[node.name] = result
        delete_unused_values(node)

        print(env.keys())

    model.graph.print_tabular()

    model.graph.lint()
    model.recompile()


def get_aten_graph_module(
    pattern: Callable,
    example_inputs: Tuple[Any, ...],
    example_kwargs: Dict[str, Any],
    dynamic_shapes: Union[Dict[str, Any], Tuple[Any], None] = None,
    is_cuda: bool = False,
) -> GraphModule:
    """
    Convert the pattern to an FX graph with decomposed aten ops.
    """
    if is_cuda:
        example_inputs = tuple([x.cuda() if isinstance(x, torch.Tensor) else x for x in example_inputs])
    aten_pattern = capture_pre_autograd_graph(
        pattern,
        example_inputs,
        example_kwargs,
        dynamic_shapes=dynamic_shapes,
    )
    aten_pattern.graph.eliminate_dead_code()
    aten_pattern.recompile()
    return aten_pattern


def get_node_name_to_scope(model: GraphModule) -> Dict[str, Tuple[str, type, int]]:
    node_name_to_scope: Dict[str, Tuple[str, type]] = {}
    submodule_to_object_type_to_cur_idx: Dict[str, Dict[Callable, int]] = \
        defaultdict(lambda: defaultdict(int))
    for n in model.graph.nodes:
        if (nn_module_stack := n.meta.get("nn_module_stack", None)) is None:
            node_name_to_scope[n.name] = [("", type(None))]
            continue

        def _normalize_path(n):
            prefix = 0
            # TODO This is non standard behavior and should be removed when we migrate off capture_pre_autograd_graph.
            if n.startswith("L['self']."):
                prefix = len("L['self'].")
            return n[prefix:]

        current_scope = []
        for bt in nn_module_stack.values():
            module_path = _normalize_path(bt[0])
            cur_object_type_idx = \
                submodule_to_object_type_to_cur_idx[module_path][n.target]
            submodule_to_object_type_to_cur_idx[module_path][n.target] += 1
            current_scope.append((module_path, bt[1], cur_object_type_idx))
        node_name_to_scope[n.name] = current_scope[-1]

    return node_name_to_scope


def print_node_scope_tabular(gm: GraphModule):
    try:
        from tabulate import tabulate
    except ImportError:
        print("`print_tabular` relies on the library `tabulate`, "
                "which could not be found on this machine. Run `pip "
                "install tabulate` to install the library.")
        raise

    node_name_to_scope = get_node_name_to_scope(gm)
    node_specs = [
        [n.op, n.name, n.target, node_name_to_scope[n.name]]
        for n in gm.graph.nodes if n.name in node_name_to_scope
    ]
    print(tabulate(node_specs,
                   headers=['opcode', 'name', 'target', 'scope']))
