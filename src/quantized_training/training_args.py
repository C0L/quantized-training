import argparse

from quantized_training.quantizer.quantizer import QuantizationSpec
from quantized_training.utils import SLURM_ARGS

__all__ = [
    "add_qspec_args",
]


qconfig_help_string = """
Input arguments as a comma-separated list. The first argument must specify the dtype.
Subsequent arguments can be specified using either abbreviations or full names.
Abbreviations and their full names:
  - qs: qscheme
  - qmax: quant_max
  - ahl: amax_history_len
  - ax: ch_axis
  - bs: block_size

Example usage:
  --params int8,qscheme=qscheme1,quant_max=127,amax_history_len=50,ch_axis=0,block_size=32
or
  --params int8,qs=qscheme1,qmax=127,ahl=50,ax=0,bs=32

Parameter details:
  - dtype (str): Data type (e.g., int8, int4, fp8_e4m3, fp8_e5m2, fp4_e2m1, posit8_1)
  - qscheme (str): Quantization scheme
  - quant_max (float): Maximum quantization value
  - amax_history_len (int): Length of the amax history (default: 50)
  - ch_axis (int): Channel axis (default: 0)
  - block_size (int): Block size (default: 32)
"""


def add_qspec_args(parser=None):
    if parser is None:
        parser = argparse.ArgumentParser(description="Run quantized inference or training.")
    #----------------------------------------------------
    # Wandb and logging arguments
    #----------------------------------------------------
    parser.add_argument(
        '--project',
        default=None,
        help='The name of the project where the new run will be sent.'
    )
    parser.add_argument(
        '--run_name',
        default=None,
        help='A short display name for this run, which is this run will be identified in the UI.'
    )
    parser.add_argument(
        '--run_id',
        default=None,
        help='A unique ID for a wandb run, used for resuming.'
    )
    parser.add_argument(
        '--sweep_config',
        default=None,
        help='Path to JSON file that stores W&B sweep configuration.'
    )
    parser.add_argument(
        '--sweep_id',
        default=None,
        help='The unique identifier for a sweep generated by W&B CLI or Python SDK.'
    )
    parser.add_argument(
        "--max_trials",
        type=int,
        default=None,
        help="The number of sweep config trials to try."
    )
    parser.add_argument(
        "--log_level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"],
        default="WARNING",
        help="Set the logging level"
    )
    parser.add_argument(
        "--log_file",
        default=None,
        help="Set the logging file. If not specified, the log will be printed to stdout."
    )
    #----------------------------------------------------
    # Training arguments
    #----------------------------------------------------
    parser.add_argument("--gpu", type=int, default=None, help="GPU to use.")
    parser.add_argument(
        "--do_train", action="store_true", help="Whether to run training"
    )
    parser.add_argument(
        "--sgd", action="store_true", help="Whether to use SGD optimizer."
    )
    parser.add_argument(
        "--warmup_ratio",
        type=float,
        default=0.0,
        help="Ratio of warmup steps in the lr scheduler."
    )
    parser.add_argument(
        "--bf16",
        action="store_true",
        help="Whether to use bf16 (mixed) precision instead of 32-bit float."
    )
    parser.add_argument(
        "--num_hidden_layers",
        type=int,
        default=None,
        help="Number of Tranformer encoder layers to use."
    )
    parser.add_argument(
        "--lora_rank",
        type=int,
        default=0,
        help="The dimension of the low-rank matrices."
    )
    parser.add_argument(
        "--lora_alpha",
        type=int,
        default=8,
        help="The scaling factor for the low-rank matrices."
    )
    parser.add_argument(
        "--target_modules",
        type=lambda x: x.split(','),
        default="query,value",
        help="The modules (for example, attention blocks) to apply the LoRA update matrices."
    )
    parser.add_argument(
        "--peft_model_id",
        default=None,
        help="Name of path of pre-trained peft adapter."
    )
    #----------------------------------------------------
    # Quantization arguments
    #----------------------------------------------------
    parser.add_argument(
        "--pt2e",
        action="store_true",
        help="Whether to use PyTorch 2 torch.export post-training static quantizaion.",
    )
    parser.add_argument(
        "--activation",
        default=None,
        help=(
            "Activation quantization specification. Comma-separated key=value pairs "
            "using abbreviations or full names. See below for details:\n" + qconfig_help_string
        ),
    )
    parser.add_argument(
        "--output_activation",
        default=None,
        help=(
            "Output activation quantization specification. Format same as activation."
        ),
    )
    parser.add_argument(
        "--weight",
        default=None,
        help=(
            "Weight quantization specification. Format same as activation."
        ),
    )
    parser.add_argument(
        "--bias",
        default=None,
        help=(
            "Bias quantization specification. Format same as activation."
        ),
    )
    parser.add_argument(
        "--error",
        default=None,
        type=QuantizationSpec.from_str,
        help=(
            "Activation gradient quantization data type and configurations. Format same as activation."
        ),
    )
    parser.add_argument(
        "--quantize_forward",
        default='gemm',
        help=(
            "Forward operations to quantize. Choose from gemm, residual, "
            "activation, layernorm, and scaling."
        ),
    )
    parser.add_argument(
        "--quantize_backprop",
        default='gemm',
        help=(
            "Backprop operations to quantize. Choose from gemm, residual, "
            "activation, layernorm, and scaling."
        ),
    )
    parser.add_argument(
        '--force_scale_power_of_two',
        action='store_true',
        help='Whether to force the scaling factor to be a power of two.',
    )
    parser.add_argument(
        '--calibration_steps',
        type=int,
        default=0,
        help='Number of calibration steps for PTQ',
    )
    parser.add_argument(
        '--convert_model',
        action='store_true',
        help='Whether to convert the model to quantized model.',
    )
    parser.add_argument(
        '--compile',
        action='store_true',
        help='Whether to generate accelerator program for the given model.',
    )
    parser.add_argument(
        "--op_fusion",
        type=lambda x: x.split(','),
        default=None,
        help="Fuse operation with previous GEMM to reduce quantization error.",
    )
    parser.add_argument(
        "--posit_exp",
        action="store_true",
        help="Whether to use posit approximated exponential function in softmax."
    )
    parser.add_argument(
        "--posit_exp_shifted",
        action="store_true",
        help="Whether to use shifted posit approximated exponential function in softmax."
    )
    parser.add_argument(
        "--posit_reciprocal",
        action="store_true",
        help="Whether to use posit approximated reciprocal function in softmax."
    )
    parser.add_argument(
        "--record_histogram",
        action="store_true",
        help="Whether to store and plot the histogram of tensor value.",
    )
    parser.add_argument(
        "--bank_width",
        type=int,
        default=None,
        help="The width of the memory bank in unit of bytes for memory planning.",
    )
    #----------------------------------------------------
    # Slurm arguments
    #----------------------------------------------------
    subparsers = parser.add_subparsers(help='sub-command help', dest='action')
    parser_slurm = subparsers.add_parser("slurm", help="slurm command help")
    for k, v in SLURM_ARGS.items():
        parser_slurm.add_argument("--" + k, **v)
    parser_bash = subparsers.add_parser("bash", help="bash command help")
    return parser
